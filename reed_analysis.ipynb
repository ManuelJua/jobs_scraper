{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and remove duplicate job listings based on 'jobId' from both JSON files\n",
    "reed_df = pd.read_json('software_developer_reed_jobs.json').drop_duplicates(subset=['jobId']).reset_index(drop=True).pipe(\n",
    "    lambda df: df[['jobId', 'employerName', 'jobTitle', 'locationName', 'minimumSalary',\n",
    "                   'maximumSalary', 'currency', 'expirationDate', 'date', 'jobDescription',\n",
    "                   'applications', 'jobUrl']]\n",
    ")\n",
    "# Convert job titles and descriptions to lowercase\n",
    "reed_df[['jobTitle', 'jobDescription']] = reed_df[[\n",
    "    'jobTitle', 'jobDescription']].applymap(str.lower)\n",
    "reed_df.shape\n",
    "\n",
    "reed_df['meanSalary'] = reed_df.apply(\n",
    "    lambda df: df[['minimumSalary', 'maximumSalary']].mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reed_df[['latitude', 'longitude']] = pd.NA\n",
    "location_df = pd.read_csv('location.csv').drop_duplicates().reset_index()\n",
    "for location in reed_df[reed_df.loc[:, 'latitude'].isna()]['locationName'].unique():\n",
    "    try:\n",
    "        reed_df.loc[reed_df['locationName'] == location,\n",
    "                    'latitude'] = location_df.loc[location_df['locationName'] == location, 'latitude'].values[0]\n",
    "        reed_df.loc[reed_df['locationName'] == location,\n",
    "                    'longitude'] = location_df.loc[location_df['locationName'] == location, 'longitude'].values[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # print(location_df.loc[location_df['locationName']==location,'latitude'].values)\n",
    "reed_df[['locationName', 'latitude', 'longitude']]\n",
    "reed_df[reed_df['latitude'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.exc import GeocoderTimedOut\n",
    "# from time import sleep\n",
    "\n",
    "# def get_coordinates_nominatim():\n",
    "#     # Initialize the Nominatim geocoder with a user agent\n",
    "#     app = Nominatim(user_agent='jobs')\n",
    "\n",
    "#     # Get unique location names from the DataFrame\n",
    "#     unique_locations = reed_df['locationName'].unique()\n",
    "\n",
    "#     # Iterate over each unique location\n",
    "#     for location in unique_locations:\n",
    "#         # Check if the latitude for the location is missing\n",
    "#         if reed_df.loc[reed_df['locationName'] == location, 'latitude'].isna().all():\n",
    "#             try:\n",
    "#                 # Attempt to geocode the location\n",
    "#                 geocode_result = app.geocode(location)\n",
    "#                 if geocode_result:\n",
    "#                     # Extract latitude and longitude from the geocode result\n",
    "#                     latitude, longitude = geocode_result.latitude, geocode_result.longitude\n",
    "#                     print(location, latitude, longitude)\n",
    "\n",
    "#                     # Update the DataFrame with the obtained coordinates\n",
    "#                     reed_df.loc[reed_df['locationName'] == location, 'latitude'] = latitude\n",
    "#                     reed_df.loc[reed_df['locationName'] == location, 'longitude'] = longitude\n",
    "\n",
    "#                     # Save the updated DataFrame to a CSV file\n",
    "#                     reed_df[['locationName', 'latitude', 'longitude']].to_csv('location.csv', index=False)\n",
    "#             except GeocoderTimedOut:\n",
    "#                 # Handle timeout exception by retrying after a delay\n",
    "#                 print(f\"Geocoding timed out for location: {location}. Retrying...\")\n",
    "#                 sleep(5)\n",
    "#                 return get_coordinates_nominatim()\n",
    "#             except Exception as e:\n",
    "#                 # Handle any other exceptions and continue with the next location\n",
    "#                 print(f\"Error geocoding location {location}: {e}\")\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of various technologies and tools used in software development,\n",
    "#  data science, AI, and cybersecurity\n",
    "technologies = ['Python', 'Django', 'Flask', 'FastAPI', 'Pyramid', 'SQL',\n",
    "                'PostgreSQL', 'MySQL', 'MongoDB', 'JavaScript', 'HTML5', 'CSS3',\n",
    "                'REST APIs', 'GraphQL', 'AWS', 'Azure', 'Docker', 'Kubernetes',\n",
    "                ' Git ', 'CI/CD', 'PyCharm', 'Jupyter', 'NumPy', 'Pandas',\n",
    "                'TensorFlow', 'Keras', 'Scikit-learn', 'Artificial Intelligence',\n",
    "                ' data ', 'pipeline AI ', 'Machine Learning', 'Deep Learning',\n",
    "                'Neural Networks', 'Natural Language Processing', 'NLP', 'LLMs',\n",
    "                'LLM', 'large Language Model', 'Computer Vision', 'Data Science',\n",
    "                'Predictive Analytics', 'Algorithm Development', 'Automation',\n",
    "                'Robotics', 'Cognitive Computing', 'Big Data', 'Data Mining',\n",
    "                'PyTorch', 'Reinforcement Learning', 'Supervised Learning',\n",
    "                'Unsupervised Learning', 'AI Ethics', 'AI Research',\n",
    "                'Model Training', 'Model Deployment', 'AI Frameworks', 'AI Tools',\n",
    "                'AI Solutions', 'HTML', 'CSS', 'React', 'Responsive Design',\n",
    "                'UI/UX Design', ' UI ', 'Adobe XD', 'Figma', 'Sketch',\n",
    "                'EmbeddingWireframing', 'Prototyping', 'User Research',\n",
    "                'Visual Design', 'Typography', 'Color Theory', 'SEO',\n",
    "                'Accessibility', 'Angular', 'Vue.js', 'Sass', 'Webpack', 'Babel',\n",
    "                'TypeScript', 'Redux', 'Bootstrap', 'Tailwind CSS',\n",
    "                'Cross-Browser Compatibility', 'Node.js', 'Express.js', 'Java ',\n",
    "                'Spring Boot', 'Ruby on Rails', 'PHP', 'Laravel', 'C#', ' .NET ',\n",
    "                'RESTful APIs', ' REST ', ' API ', ' APIs ', 'Authentication',\n",
    "                'Authorization', 'Microservices', 'Cyber Security Analyst',\n",
    "                'Cyber Security', 'Information Security Specialist',\n",
    "                'Network Security Engineer', 'Penetration Tester',\n",
    "                'Security Consultant', 'Security Operations Center (SOC) Analyst',\n",
    "                'Incident Response Specialist', 'Cyber Threat Analyst',\n",
    "                'Security Architect', 'Vulnerability Assessor',\n",
    "                'Risk Management Specialist', 'Security Compliance Analyst',\n",
    "                'Ethical Hacker', 'Digital Forensics Investigator',\n",
    "                'Cloud Security Engineer', 'Application Security Engineer',\n",
    "                'Identity and Access Management (IAM) Specialist',\n",
    "                'Security Software Developer',\n",
    "                'Chief Information Security Officer (CISO)',\n",
    "                'Security Awareness Trainer', 'Cyber', 'backend', 'frontend',\n",
    "                'Apache Spark', 'Hadoop', 'Kafka', 'Airflow', 'Informatica',\n",
    "                'Talend', 'Snowflake', 'Redshift', 'Scala', 'Java', 'Tableau',\n",
    "                'Power BI', '.NET Framework', 'ASP.NET', 'Entity Framework',\n",
    "                'LINQ', 'Visual Studio', 'Xamarin',\n",
    "                'WPF (Windows Presentation Foundation)',\n",
    "                'WCF (Windows Communication Foundation)', 'SQL Server', 'NuGet',\n",
    "                'ReSharper', 'DevOps', 'Cloud', 'Solutions Architect',\n",
    "                'Security', 'Data Engineer',\n",
    "                'AI Engineer', 'Kubernetes Service (AKS)', 'lambda functions',\n",
    "                'Logic Apps', 'Synapse Analytics', 'Active Directory',\n",
    "                'Virtual Machines', 'Storage', 'Networking', 'SQL Database',\n",
    "                'Cosmos DB', 'Site Recovery', 'Backup', 'Policy',\n",
    "                'Resource Manager', 'Service Fabric', 'Event Hubs', 'IoT Hub', ' IoT '\n",
    "                'Cognitive Services', 'EC2', 'S3', 'CloudFront', 'ElastiCache',\n",
    "                'S3 Glacier', 'SageMaker', ' RDS ', ' IAM ', ' EBS ', 'Lambda', ' EFS ',\n",
    "                'SNS', 'VPC', 'AWS Auto-Scaling', 'SQS', 'Elastic Beanstalk',\n",
    "                'DynamoDB', 'Cloud Directory', 'Cognito', 'Inspector', 'Aurora',\n",
    "                'CloudWatch', 'AWS Firewall Manager', 'AWS KMS', 'LightSail', ' Web ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the frequency of technology words\n",
    "frecuency_tech_words = pd.DataFrame(\n",
    "    0, index=technologies, columns=technologies)\n",
    "\n",
    "# Convert job descriptions to lowercase once\n",
    "reed_df['jobDescription'] = reed_df['jobDescription'].str.lower()\n",
    "\n",
    "# Create a dictionary to store the presence of each technology word in job descriptions\n",
    "word_presence = {word: reed_df['jobDescription'].str.contains(\n",
    "    word.lower()) for word in technologies}\n",
    "\n",
    "# Iterate over each pair of technology words\n",
    "for word_column in technologies:\n",
    "    for word_row in technologies:\n",
    "        # Count the number of job descriptions that contain both technology words\n",
    "        num_true = (word_presence[word_column] & word_presence[word_row]).sum()\n",
    "        # Store the count in the DataFrame\n",
    "        frecuency_tech_words.loc[word_row, word_column] = num_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "x_y_label_size = 12\n",
    "title_size = 15\n",
    "\n",
    "# Plot the most popular keywords in job descriptions\n",
    "frecuency_tech_words.max().sort_values(ascending=False).head(\n",
    "    20).plot(kind='barh', ax=ax[0, 0], color='black')\n",
    "ax[0, 0].set_title(\n",
    "    \"Most popular keywords in job descriptions\", fontsize=title_size)\n",
    "ax[0, 0].tick_params(axis='y', labelsize=x_y_label_size)\n",
    "ax[0, 0].tick_params(axis='x', labelsize=x_y_label_size)\n",
    "ax[0, 0].invert_yaxis()\n",
    "ax[0, 0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Define keywords, axes, and colors\n",
    "keywords = ['Python', 'Data Engineer', ' Web ']\n",
    "axes = [ax[0, 1], ax[1, 0], ax[1, 1]]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "# Plot the most popular keywords in job descriptions that also include specific keywords\n",
    "for keyword, axis, color in zip(keywords, axes, colors):\n",
    "    frecuency_tech_words[keyword].sort_values(ascending=False).head(\n",
    "        21)[1:].plot(kind='barh', ax=axis, color=color)\n",
    "    axis.set_title(\n",
    "        f\"Most popular keywords along with '{keyword}'\", fontsize=title_size, color=color)\n",
    "    axis.tick_params(axis='y', labelsize=x_y_label_size)\n",
    "    axis.tick_params(axis='x', labelsize=x_y_label_size)\n",
    "    axis.invert_yaxis()\n",
    "    axis.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ''\n",
    "reed_df['expirationDate'] = pd.to_datetime(\n",
    "    reed_df['expirationDate'], format=\"%d/%m/%Y\")\n",
    "reed_df['date'] = pd.to_datetime(reed_df['date'], format=\"%d-%m-%Y\")\n",
    "juniors = reed_df[reed_df['jobTitle'].str.lower().str.contains(\n",
    "    pat=f\"(entry|junior|graduate|apprentice).*{keyword}\", regex=True)]['date'].value_counts().sort_index()\n",
    "juniors.plot(kind='bar')\n",
    "plt.xticks(ticks=range(0, juniors.shape[0], 5), labels=juniors.index.map(\n",
    "    lambda x: x.date)[::5])\n",
    "plt.title(\"Junior position vacancies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def junior_vacancies_bar_plot(technology_keyword=''):\n",
    "    # Filter the DataFrame for job titles containing 'junior', 'entry', or 'graduate' and the technology keyword\n",
    "    reed_df['jobTitle'] = reed_df['jobTitle'].str.lower()\n",
    "    technology_keyword = technology_keyword.lower()\n",
    "    juniors = (\n",
    "        reed_df[reed_df['jobTitle']\n",
    "                .apply(lambda df: True if (('junior' or 'entry' or 'graduate') and technology_keyword) in df else False)]\n",
    "        ['date']\n",
    "        .value_counts()\n",
    "    )\n",
    "\n",
    "    # Convert the index to datetime format\n",
    "    juniors.index = pd.to_datetime(juniors.index, format=\"%d/%m/%Y\")\n",
    "\n",
    "    # Create a bar plot of the number of vacancies by date\n",
    "    sns.barplot(data=juniors.sort_index())\n",
    "\n",
    "    # Rotate the x-axis labels for better readability\n",
    "    plt.xticks(range(0, len(juniors), 5), rotation=90)\n",
    "\n",
    "    # Set the title and labels of the plot\n",
    "    plt.title(f\"Junior position vacancies with '{technology_keyword}'\")\n",
    "    plt.ylabel(\"Number of vacancies published daily\")\n",
    "    plt.xlabel(\"Day\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_vacancies_bar_plot(technology_keyword='Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_salary_dist(df, salary, title):\n",
    "    # Filter the DataFrame to include only rows where 'salary' is not null and greater than 10000\n",
    "    data = df[df[salary].gt(10000)]\n",
    "\n",
    "    # Create a distribution plot for the 'salary' column\n",
    "    sns.displot(data=data, x=salary)\n",
    "\n",
    "    # Set the x-axis ticks from 0 to 150000 with an interval of 10000 and rotate the labels by 90 degrees\n",
    "    plt.xticks(range(0, 150000, 10000), rotation=90)\n",
    "\n",
    "    # Set the x-axis limit from 0 to 150000\n",
    "    xmax = data[salary].mean() + 4*data[salary].std()\n",
    "    plt.xlim(0, xmax)\n",
    "\n",
    "    # Draw a vertical line at the median value of the 'salary' column\n",
    "    median_salary = data[salary].median()\n",
    "    plt.axvline(x=median_salary, color='r', linestyle='--')\n",
    "\n",
    "    # Add text at the position (1.1 * median, y_median_pos) displaying the median value\n",
    "    y_median_pos = data[salary].value_counts().max()\n",
    "    plt.text(x=1.1 * data[salary].median(), y=1.5*y_median_pos,\n",
    "             s=f\"median={data[salary].median()}\", color='r')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    plt.title(title)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_salary_dist(reed_df, 'meanSalary',\n",
    "                 title=\"Software Developers: Salary Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_salary_dist(\n",
    "    reed_df[reed_df['jobTitle'].str.lower().str.contains(pat=r\"junior|entry|graduate|apprentice\", regex=True)], \"meanSalary\", title=\"Junior Software Developers: Salary Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_salary_dist(\n",
    "    (reed_df[reed_df['jobTitle'].str.lower().str.contains(pat=r\"senior|principal|semi-senior|lead\", regex=True)]), \"meanSalary\", title=\"Senior Software Developers: Salary Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact, HBox\n",
    "from IPython.display import display\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_scatter_map_with_color_scaled_clusters(df, keyword=''):\n",
    "    # Create a continuous color scale\n",
    "    colorscale = px.colors.sequential.Viridis\n",
    "\n",
    "    df = df[df['jobDescription'].str.contains(\n",
    "        pat=r\"{}\".format(keyword).lower(), regex=True, case=False)]\n",
    "    fig = px.scatter_map(data_frame=df, lat='latitude', lon='longitude',\n",
    "                         hover_name='jobTitle', hover_data='locationName')\n",
    "    fig.update_layout(\n",
    "        map=dict(\n",
    "            style=\"open-street-map\",\n",
    "            zoom=4,\n",
    "            center=dict(lat=df['latitude'].mean(), lon=df['longitude'].mean())\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=600,\n",
    "        margin=dict(l=0, r=0, t=0, b=0)\n",
    "    )\n",
    "\n",
    "    # Define cluster steps and corresponding colors\n",
    "    steps = [0, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "    colors = [colorscale[int(i)] for i in np.linspace(\n",
    "        0, len(colorscale)-1, len(steps))]\n",
    "\n",
    "    # Enable clustering with color steps\n",
    "    fig.update_traces(\n",
    "        cluster=dict(\n",
    "            enabled=True,\n",
    "            step=steps,\n",
    "            color=colors,\n",
    "            opacity=0.7,\n",
    "            size=list(range(10, 35, 5)),  # Increase size with cluster size\n",
    "            maxzoom=15,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show(renderer='notebook')\n",
    "\n",
    "\n",
    "# Create a text box widget for keyword input\n",
    "keyword_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a keyword to filter jobs',\n",
    "    description='Keyword:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the widget and update the plot based on the input\n",
    "\n",
    "\n",
    "def update_plot(keyword):\n",
    "    create_scatter_map_with_color_scaled_clusters(reed_df, keyword)\n",
    "\n",
    "\n",
    "interact(update_plot, keyword=keyword_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reed_df['employerName'].value_counts().head(\n",
    "    20).plot(kind='barh', figsize=(5, 5))\n",
    "plt.title(\"Publications by biggest employers\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import re\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, callback\n",
    "\n",
    "\n",
    "def create_distribution_bar_plot(df, keyword=''):\n",
    "    # Filter the DataFrame based on the keyword\n",
    "    df = df[df['jobDescription'].str.contains(\n",
    "        pat=keyword, regex=True, case=False)]\n",
    "\n",
    "    # Create the distribution plot\n",
    "    print(f\"{df.shape[0]} results\")\n",
    "    if df.shape[0] > 0:\n",
    "        median_salary = df['meanSalary'].median()\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.histplot(data=df, x='meanSalary')\n",
    "        plt.axvline(x=median_salary, color='r', linestyle='--')\n",
    "        plt.text(x=median_salary+1000,\n",
    "                 y=df['meanSalary'].value_counts().max(),\n",
    "                 s=f\"Median salary: Â£{int(median_salary)}\",\n",
    "                 color='r')\n",
    "        plt.xlim(2000, df['meanSalary'].quantile(0.99))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Create a text box widget for keyword input\n",
    "keyword_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a keyword to filter jobs',\n",
    "    description='Keyword:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='300px', height='30px')\n",
    "\n",
    "\n",
    ")\n",
    "# Display the widget and update the plot based on the input\n",
    "\n",
    "\n",
    "def update_plot(keyword):\n",
    "    create_distribution_bar_plot(reed_df, keyword)\n",
    "\n",
    "\n",
    "interact(update_plot, keyword=keyword_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Function to create a Folium map with clustered points\n",
    "def create_folium_map(df, keyword=''):\n",
    "    # Filter DataFrame by keyword in jobDescription\n",
    "    filtered_df = df[df['jobDescription'].str.contains(pat=keyword, case=False, na=False) & df['latitude'].notna(\n",
    "    ) & df['longitude'].notna() & df['locationName'].notna()]\n",
    "\n",
    "    # If no results, use all rows with valid latitude\n",
    "    if filtered_df.empty:\n",
    "        filtered_df = df[df['latitude'].notna()]\n",
    "\n",
    "    # Initialize a Folium map centered on the mean latitude and longitude\n",
    "    m = folium.Map(location=[filtered_df['latitude'].mean(\n",
    "    ), filtered_df['longitude'].mean()], zoom_start=4)\n",
    "\n",
    "    # Initialize a MarkerCluster object\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    # Add markers to the cluster\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=f\"Job: {row['jobTitle']}<br><br>Location: {row['locationName']}\"\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Function to update the map based on the keyword input\n",
    "\n",
    "\n",
    "def update_map(keyword):\n",
    "    m = create_folium_map(reed_df, keyword)\n",
    "    display(m)\n",
    "\n",
    "\n",
    "# Create a text box widget for keyword input\n",
    "keyword_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a keyword to filter jobs',\n",
    "    description='Keyword:',\n",
    "    continuous_update=False  # This enables debouncing\n",
    ")\n",
    "\n",
    "# Use interact to link the widget to the update_map function\n",
    "interact(update_map, keyword=keyword_widget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_jooble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
